{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CE-VAE: Underwater Image Enhancement with Custom Datasets\n",
        "\n",
        "This notebook allows you to use the CE-VAE model for underwater image enhancement with your own datasets.\n",
        "\n",
        "**Before running:**\n",
        "1. Go to Runtime → Change runtime type → Select GPU\n",
        "2. Prepare your dataset with the folder structure described below"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup Environment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/priyanshuharshbodhi1/ce-vae-underwater-image-enhancement.git\n",
        "%cd ce-vae-underwater-image-enhancement"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Mount Google Drive (for dataset access)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Dataset Preparation\n",
        "\n",
        "Your dataset should have this structure:\n",
        "```\n",
        "your_dataset/\n",
        "├── train/\n",
        "│   ├── GT/      # Ground truth images\n",
        "│   └── input/   # Degraded images\n",
        "└── val/\n",
        "    ├── GT/\n",
        "    └── input/\n",
        "```\n",
        "\n",
        "**Set your dataset path below:**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CONFIGURE YOUR DATASET PATH HERE ======\n",
        "# Option 1: Dataset in Google Drive\n",
        "DATASET_PATH = \"/content/drive/MyDrive/my_underwater_dataset\"\n",
        "\n",
        "# Option 2: Upload directly (uncomment and run separately)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()  # Upload a zip file\n",
        "# !unzip your_dataset.zip -d /content/dataset\n",
        "# DATASET_PATH = \"/content/dataset\"\n",
        "\n",
        "# Verify the dataset structure\n",
        "import os\n",
        "print(f\"Dataset path: {DATASET_PATH}\")\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(\"\\nDataset structure:\")\n",
        "    for root, dirs, files in os.walk(DATASET_PATH):\n",
        "        level = root.replace(DATASET_PATH, '').count(os.sep)\n",
        "        indent = ' ' * 2 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        if level < 2:  # Only show first 2 levels\n",
        "            subindent = ' ' * 2 * (level + 1)\n",
        "            file_count = len(files)\n",
        "            if file_count > 0:\n",
        "                print(f\"{subindent}({file_count} files)\")\n",
        "else:\n",
        "    print(\"ERROR: Dataset path does not exist!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset text files\n",
        "!bash scripts/generate_dataset_txt.sh \"{DATASET_PATH}\"\n",
        "\n",
        "# Check generated files\n",
        "!echo \"Generated files:\"\n",
        "!ls -la data/*.txt 2>/dev/null || echo \"No text files found. Make sure your dataset has the correct structure.\""
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Download Pre-trained Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet pre-trained weights (for training from scratch)\n",
        "# You need to get the download link from the README and use gdown or wget\n",
        "\n",
        "!mkdir -p data\n",
        "\n",
        "# Option 1: If you have the file in Google Drive\n",
        "# !cp \"/content/drive/MyDrive/imagenet-pre-trained-cevae.ckpt\" data/\n",
        "\n",
        "# Option 2: Download using gdown (if you have the Google Drive file ID)\n",
        "# !pip install gdown -q\n",
        "# !gdown \"YOUR_GDRIVE_FILE_ID\" -O data/imagenet-pre-trained-cevae.ckpt\n",
        "\n",
        "# Option 3: Download LSUI pre-trained model for inference\n",
        "# !gdown \"YOUR_LSUI_CHECKPOINT_ID\" -O data/lsui-pretrained.ckpt\n",
        "\n",
        "print(\"Available checkpoints in data/:\")\n",
        "!ls -la data/*.ckpt 2>/dev/null || echo \"No checkpoints found. Please download the pre-trained model.\""
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Create Custom Config (if training)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom config for your dataset\n",
        "custom_config = \"\"\"\n",
        "#Vector Capsule VAE - Custom Dataset Config\n",
        "model:\n",
        "  target: src.models.cevae.CEVAE\n",
        "  params:\n",
        "    discriminator: False\n",
        "    ckpt_path: data/imagenet-pre-trained-cevae.ckpt\n",
        "    embed_dim: 256\n",
        "    ddconfig:\n",
        "      double_z: False\n",
        "      z_channels: 256\n",
        "      resolution: 256\n",
        "      in_channels: 3\n",
        "      out_ch: 3\n",
        "      ch: 128\n",
        "      ch_mult: [1, 1, 2, 2, 4]\n",
        "      num_res_blocks: 2\n",
        "      attn_resolutions: [16]\n",
        "      dropout: 0.0\n",
        "\n",
        "    lossconfig:\n",
        "      target: src.modules.losses.combined.ReconstructionLossWithDiscriminator\n",
        "      params:\n",
        "        pixelloss_weight: 10.0\n",
        "        perceptual_weight: 1.0\n",
        "        gdl_loss_weight: 0.0\n",
        "        color_loss_weight: 0.0\n",
        "        ssim_loss_weight: 1.0\n",
        "        disc_enabled: False\n",
        "\n",
        "    optimizer:\n",
        "      base_learning_rate: 4.5e-6\n",
        "\n",
        "lightning:\n",
        "  trainer:\n",
        "    max_epochs: 100  # Adjust as needed\n",
        "    accelerator: gpu\n",
        "    devices: 1\n",
        "    check_val_every_n_epoch: 10\n",
        "\n",
        "data:\n",
        "  target: src.data.dataset_wrapper.DataModuleFromConfig\n",
        "  params:\n",
        "    dataset_name: \"CustomDataset\"\n",
        "    train_batch_size: 4  # Reduce if OOM\n",
        "    val_batch_size: 8\n",
        "    num_workers: 4\n",
        "    train:\n",
        "      target: src.data.image_enhancement.DatasetTrainFromImageFileList\n",
        "      params:\n",
        "        training_images_list_file: data/LSUI_train_input.txt\n",
        "        target_images_list_file: data/LSUI_train_target.txt\n",
        "        random_crop: True\n",
        "        random_flip: True\n",
        "        color_jitter:\n",
        "          brightness: [0.9, 1.1]\n",
        "          contrast: [0.9, 1.1]\n",
        "          saturation: [0.9, 1.1]\n",
        "          hue: [-0.02, 0.02]\n",
        "        max_size: 288\n",
        "        size: 256\n",
        "    validation:\n",
        "      target: src.data.image_enhancement.DatasetTestFromImageFileList\n",
        "      params:\n",
        "        test_images_list_file: data/LSUI_val_input.txt\n",
        "        test_target_images_list_file: data/LSUI_val_target.txt\n",
        "        size: 256\n",
        "    test:\n",
        "      target: src.data.image_enhancement.DatasetTestFromImageFileList\n",
        "      params:\n",
        "        test_images_list_file: data/LSUI_val_input.txt\n",
        "        test_target_images_list_file: data/LSUI_val_target.txt\n",
        "        size: 256\n",
        "\"\"\"\n",
        "\n",
        "with open('configs/cevae_custom.yaml', 'w') as f:\n",
        "    f.write(custom_config)\n",
        "\n",
        "print(\"Custom config created: configs/cevae_custom.yaml\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Training (Optional)\n",
        "\n",
        "Run this if you want to train on your dataset. Skip to section 7 for inference only."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Set wandb to offline mode (or login with: wandb login)\n",
        "!wandb offline\n",
        "\n",
        "# Train the model\n",
        "!python main.py --config configs/cevae_custom.yaml"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Inference: Enhance Your Images"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== CONFIGURE PATHS ======\n",
        "# Path to the checkpoint (use pre-trained or your trained model)\n",
        "CHECKPOINT_PATH = \"data/lsui-pretrained.ckpt\"  # Change this to your checkpoint\n",
        "\n",
        "# Path to images you want to enhance\n",
        "INPUT_IMAGES_PATH = f\"{DATASET_PATH}/val/input\"  # or any folder with images\n",
        "\n",
        "# Where to save enhanced images\n",
        "OUTPUT_PATH = \"/content/enhanced_images\"\n",
        "\n",
        "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
        "print(f\"Input images: {INPUT_IMAGES_PATH}\")\n",
        "print(f\"Output folder: {OUTPUT_PATH}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run inference\n",
        "!python test.py \\\n",
        "    --config configs/cevae_E2E_lsui.yaml \\\n",
        "    --checkpoint \"{CHECKPOINT_PATH}\" \\\n",
        "    --data-path \"{INPUT_IMAGES_PATH}\" \\\n",
        "    --output-path \"{OUTPUT_PATH}\" \\\n",
        "    --batch-size 4 \\\n",
        "    --device cuda:0"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. View Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Get list of enhanced images\n",
        "enhanced_images = [f for f in os.listdir(OUTPUT_PATH) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Display up to 4 random samples\n",
        "num_samples = min(4, len(enhanced_images))\n",
        "samples = random.sample(enhanced_images, num_samples)\n",
        "\n",
        "fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n",
        "if num_samples == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for idx, img_name in enumerate(samples):\n",
        "    # Enhanced image\n",
        "    enhanced = Image.open(os.path.join(OUTPUT_PATH, img_name))\n",
        "    \n",
        "    # Try to find corresponding input image\n",
        "    input_path = os.path.join(INPUT_IMAGES_PATH, img_name)\n",
        "    if os.path.exists(input_path):\n",
        "        original = Image.open(input_path)\n",
        "        axes[idx][0].imshow(original)\n",
        "        axes[idx][0].set_title(f'Input: {img_name}')\n",
        "    else:\n",
        "        axes[idx][0].text(0.5, 0.5, 'Input not found', ha='center')\n",
        "        axes[idx][0].set_title('Input')\n",
        "    axes[idx][0].axis('off')\n",
        "    \n",
        "    axes[idx][1].imshow(enhanced)\n",
        "    axes[idx][1].set_title(f'Enhanced: {img_name}')\n",
        "    axes[idx][1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nTotal enhanced images: {len(enhanced_images)}\")\n",
        "print(f\"Saved to: {OUTPUT_PATH}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Compute Quality Metrics"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from src.metrics import compute\n",
        "import os\n",
        "\n",
        "# Path to ground truth images (for reference metrics)\n",
        "GT_PATH = f\"{DATASET_PATH}/val/GT\"  # Set to None if no GT available\n",
        "\n",
        "# Compute metrics for all enhanced images\n",
        "all_metrics = []\n",
        "enhanced_images = [f for f in os.listdir(OUTPUT_PATH) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "for img_name in enhanced_images[:10]:  # Limit to first 10 for speed\n",
        "    enhanced_path = os.path.join(OUTPUT_PATH, img_name)\n",
        "    enhanced_img = np.array(Image.open(enhanced_path).convert('RGB'))\n",
        "    \n",
        "    # Load GT if available\n",
        "    gt_img = None\n",
        "    if GT_PATH and os.path.exists(os.path.join(GT_PATH, img_name)):\n",
        "        gt_img = np.array(Image.open(os.path.join(GT_PATH, img_name)).convert('RGB'))\n",
        "        # Resize GT to match enhanced if needed\n",
        "        if gt_img.shape != enhanced_img.shape:\n",
        "            from PIL import Image as PILImage\n",
        "            gt_img = np.array(PILImage.fromarray(gt_img).resize((enhanced_img.shape[1], enhanced_img.shape[0])))\n",
        "    \n",
        "    metrics = compute(enhanced_img, gt_img, gt_metrics=False)\n",
        "    all_metrics.append(metrics)\n",
        "    print(f\"{img_name}: PSNR={metrics['psnr']:.2f}, SSIM={metrics['ssim']:.4f}, UIQM={metrics['uiqm']:.4f}, UCIQE={metrics['uciqe']:.4f}\")\n",
        "\n",
        "# Average metrics\n",
        "if all_metrics:\n",
        "    print(\"\\n=== Average Metrics ===\")\n",
        "    for key in ['psnr', 'ssim', 'uiqm', 'uciqe', 'niqe']:\n",
        "        values = [m[key] for m in all_metrics if m[key] != -1]\n",
        "        if values:\n",
        "            print(f\"{key.upper()}: {np.mean(values):.4f} ± {np.std(values):.4f}\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download Results"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip and download enhanced images\n",
        "!zip -r enhanced_images.zip \"{OUTPUT_PATH}\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download('enhanced_images.zip')"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or copy to Google Drive\n",
        "!cp -r \"{OUTPUT_PATH}\" \"/content/drive/MyDrive/enhanced_underwater_images\"\n",
        "print(\"Copied to Google Drive!\")"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
